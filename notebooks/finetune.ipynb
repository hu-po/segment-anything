{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from segment_anything.modeling import (\n",
    "    ImageEncoderViT,\n",
    "    MaskDecoder,\n",
    "    PromptEncoder,\n",
    "    Sam,\n",
    "    TwoWayTransformer,\n",
    ")\n",
    "from segment_anything.modeling.sam import Sam\n",
    "from segment_anything import sam_model_registry\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class FragmentDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Directory containing the datasets\n",
    "        data_dir: str,\n",
    "        # Filenames of the images we'll use\n",
    "        image_mask_filename='mask.png',\n",
    "        image_labels_filename='inklabels.png',\n",
    "        slices_dir_filename='surface_volume',\n",
    "        # Expected slices per fragment\n",
    "        crop_size: Tuple[int] = (3, 256, 256),\n",
    "        label_size: Tuple[int] = (256, 256),\n",
    "        # Number of subvolumes to extract from each image\n",
    "        num_samples: int = 2,\n",
    "        # Mean and STD for sampling slices\n",
    "        mean: float = 30,\n",
    "        std_dev: float = 10,\n",
    "        # Min and Max values for sampling slices\n",
    "        min_value: int = 0,\n",
    "        max_value: int = 65,\n",
    "        # Image resize ratio\n",
    "        resize_ratio: float = 1.0,\n",
    "        # Training vs Testing mode\n",
    "        train: bool = True,\n",
    "        # Device to use\n",
    "        device: str = 'cuda',\n",
    "        # Number of points to sample per crop\n",
    "        points_per_crop: int = 20,\n",
    "    ):\n",
    "        print('Initializing Dataset')\n",
    "        self.device = device\n",
    "        # Train mode also loads the labels\n",
    "        self.train = train\n",
    "        self.points_per_crop = points_per_crop\n",
    "        # Resize ratio reduces the size of the image\n",
    "        self.resize_ratio = resize_ratio\n",
    "        assert os.path.exists(\n",
    "            data_dir), f\"Data directory {data_dir} does not exist\"\n",
    "        # Open Mask image\n",
    "        _image_mask_filepath = os.path.join(data_dir, image_mask_filename)\n",
    "        _mask_img = cv2.imread(_image_mask_filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        # Get original size and resized size\n",
    "        self.height_original = _mask_img.shape[0]\n",
    "        self.width_original = _mask_img.shape[1]\n",
    "        self.height_resize = int(self.height_original * self.resize_ratio)\n",
    "        self.width_resize = int(self.width_original * self.resize_ratio)\n",
    "        self.depth_crop = crop_size[0]\n",
    "        self.height_crop = crop_size[1]\n",
    "        self.width_crop = crop_size[2]\n",
    "        self.label_size = label_size\n",
    "        mask_img = cv2.resize(_mask_img, (self.width_resize, self.height_resize))\n",
    "        self.mask = torch.from_numpy(np.array(mask_img)).to(dtype=torch.float32)\n",
    "        self.mask = torch.nn.functional.pad(\n",
    "            self.mask,\n",
    "            (\n",
    "                self.height_crop // 2, self.height_crop // 2,\n",
    "                self.width_crop // 2, self.width_crop // 2,\n",
    "            ),\n",
    "            mode='constant',\n",
    "            value=0,\n",
    "        )\n",
    "        if self.train:\n",
    "            # Open Label image\n",
    "            _image_labels_filepath = os.path.join(data_dir, image_labels_filename)\n",
    "            _labels_img = cv2.imread(_image_labels_filepath, cv2.IMREAD_GRAYSCALE)\n",
    "            labels_img = cv2.resize(_labels_img, (self.width_resize, self.height_resize))\n",
    "            self.labels = torch.from_numpy(np.array(labels_img)).to(dtype=torch.float32)\n",
    "            self.labels = torch.nn.functional.pad(\n",
    "                self.labels,\n",
    "                (\n",
    "                    self.height_crop // 2, self.height_crop // 2,\n",
    "                    self.width_crop // 2, self.width_crop // 2,\n",
    "                ),\n",
    "                mode='constant',\n",
    "                value=0,\n",
    "            )\n",
    "\n",
    "        self.slice_dir = os.path.join(data_dir, slices_dir_filename)\n",
    "        self.num_samples = num_samples\n",
    "        self.indices_start = np.zeros((num_samples, 3), dtype=np.int64)\n",
    "        self.indices_end = np.zeros((num_samples, 3), dtype=np.int64)\n",
    "        for i in range(num_samples):\n",
    "\n",
    "            # Select a random starting point for the subvolume\n",
    "            d_start = int(np.clip(np.random.normal(mean, std_dev), min_value, max_value - 2))\n",
    "            h_start = np.random.randint(self.height_resize // 2,\n",
    "                                        self.height_resize - self.height_crop // 2)\n",
    "            w_start = np.random.randint(self.width_resize // 2,\n",
    "                                        self.width_resize - self.width_crop // 2)\n",
    "\n",
    "            # Populate the indices matrices\n",
    "            self.indices_start[i, :] = [d_start, h_start, w_start]\n",
    "            self.indices_end[i, :] = [\n",
    "                d_start + self.depth_crop,\n",
    "                h_start + self.height_crop,\n",
    "                w_start + self.width_crop,\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Should Return:\n",
    "\n",
    "          batched_input (list(dict)): A list over input images, each a\n",
    "            dictionary with the following keys. A prompt key can be\n",
    "            excluded if it is not present.\n",
    "              'image': The image as a torch tensor in 3xHxW format,\n",
    "                already transformed for input to the model.\n",
    "              'original_size': (tuple(int, int)) The original size of\n",
    "                the image before transformation, as (H, W).\n",
    "              'point_coords': (torch.Tensor) Batched point prompts for\n",
    "                this image, with shape BxNx2. Already transformed to the\n",
    "                input frame of the model.\n",
    "              'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
    "                with shape BxN.\n",
    "              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n",
    "                Already transformed to the input frame of the model.\n",
    "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
    "                in the form Bx1xHxW.\n",
    "          multimask_output (bool): Whether the model should predict multiple\n",
    "            disambiguating masks, or return a single mask.\n",
    "\n",
    "        \"\"\"\n",
    "        start = self.indices_start[idx, :]\n",
    "        end = self.indices_end[idx, :]\n",
    "        crop = torch.zeros((\n",
    "            self.depth_crop,\n",
    "            self.height_crop,\n",
    "            self.width_crop,\n",
    "        ), dtype=torch.float32)\n",
    "        for i, slice in enumerate(range(start[0], end[0])):\n",
    "            slice_filepath = os.path.join(self.slice_dir, f\"{slice:02d}.tif\")\n",
    "            cv2_img = cv2.imread(slice_filepath, cv2.IMREAD_GRAYSCALE)\n",
    "            cv2_img = cv2.resize(cv2_img, (self.width_resize, self.height_resize))\n",
    "            cv2_img = torch.from_numpy(np.array(cv2_img)).to(dtype=torch.float32)\n",
    "            cv2_img = torch.nn.functional.pad(\n",
    "                cv2_img,\n",
    "                (\n",
    "                    self.height_crop // 2, self.height_crop // 2,\n",
    "                    self.width_crop // 2, self.width_crop // 2,\n",
    "                ),\n",
    "                mode='constant',\n",
    "                value=0,\n",
    "            )\n",
    "            crop[i, :, :] = cv2_img[start[1]:end[1], start[2]:end[2]]\n",
    "        image = crop.to(device=self.device)\n",
    "\n",
    "        # Choose N random points within the crop\n",
    "        point_coords = torch.zeros((self.points_per_crop, 2), dtype=torch.long)\n",
    "        point_labels = torch.zeros(self.points_per_crop, dtype=torch.long)\n",
    "        for i in range(self.points_per_crop):\n",
    "            point_coords[i, 0] = np.random.randint(0, self.height_crop)\n",
    "            point_coords[i, 1] = np.random.randint(0, self.width_crop)\n",
    "            point_labels[i] = self.labels[\n",
    "                start[1] + point_coords[i, 0],\n",
    "                start[2] + point_coords[i, 1],\n",
    "            ]\n",
    "        point_coords = point_coords.to(device=self.device)\n",
    "        point_labels = point_labels.to(device=self.device)\n",
    "        if self.train:\n",
    "            labels = self.labels[\n",
    "                    start[1]:end[1],\n",
    "                    start[2]:end[2],\n",
    "            ]\n",
    "            # convert to cv2 image\n",
    "            labels = labels.numpy()\n",
    "            labels = cv2.resize(labels, self.label_size)\n",
    "            labels = torch.from_numpy(labels).to(dtype=torch.float32)\n",
    "            labels = labels.unsqueeze(0).clone().to(device=self.device)\n",
    "            return image, point_coords, point_labels, labels\n",
    "        else:\n",
    "            return image, point_coords, point_labels\n",
    "\n",
    "def train_valid(\n",
    "    output_dir: str = \"output/train\",\n",
    "    train_dir: str = \"/home/tren/dev/ashenvenus/data/split_train/1\",\n",
    "    valid_dir: str = \"/home/tren/dev/ashenvenus/data/split_valid/1\",\n",
    "    model: str = \"vit_b\",\n",
    "    weights_filepath: str = \"/home/tren/dev/segment-anything/models/sam_vit_b_01ec64.pth\",\n",
    "    num_samples_train: int = 2,\n",
    "    num_samples_valid: int = 2,\n",
    "    batch_size: int = 1,\n",
    "    optimizer: str = \"adam\",\n",
    "    lr: float = 1e-4,\n",
    "    wd: float = 1e-4,\n",
    "    image_augs: bool = False,\n",
    "    crop_size: Tuple[int] = (3, 1024, 1024),\n",
    "    resize_ratio: float = 1.0,\n",
    "    num_epochs: int = 2,\n",
    "    save_model: bool = True,\n",
    "    device: str = \"cpu\",  # \"cuda:0\"\n",
    "    **kwargs,\n",
    "):\n",
    "    train_dataset = FragmentDataset(\n",
    "        data_dir=train_dir,\n",
    "        num_samples=num_samples_train,\n",
    "        crop_size=crop_size,\n",
    "        resize_ratio=resize_ratio,\n",
    "        train=True,\n",
    "        device=device,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # pin_memory=True,\n",
    "    )\n",
    "    valid_dataset = FragmentDataset(\n",
    "        data_dir=valid_dir,\n",
    "        num_samples=num_samples_valid,\n",
    "        crop_size=crop_size,\n",
    "        resize_ratio=resize_ratio,\n",
    "        train=True,\n",
    "        device=device,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        # pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = sam_model_registry[model](checkpoint=weights_filepath)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=output_dir)\n",
    "\n",
    "    step = 0\n",
    "    num_epochs = 2\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loader = tqdm(train_loader)\n",
    "        for batch in loader:\n",
    "            images, point_coords, point_labels, labels = batch\n",
    "            image_embeddings = model.image_encoder(images)\n",
    "            sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                points=(point_coords, point_labels),\n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "            )\n",
    "            # Something goes on here for batch sizes greater than 1\n",
    "            low_res_masks, iou_predictions = model.mask_decoder(\n",
    "                image_embeddings=image_embeddings,\n",
    "                image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "            loss = loss_fn(low_res_masks, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "\n",
    "            loss_name = f\"Train.{loss_fn.__class__.__name__}\"\n",
    "            writer.add_scalar(loss_name, loss.item(), step)\n",
    "            loader.set_postfix_str(f\"{loss_name}: {loss.item():.4f}\")\n",
    "        \n",
    "        if save_model:\n",
    "            _model_filepath = os.path.join(output_dir, f\"model_{epoch}.pth\")\n",
    "            print(f\"Saving model to {_model_filepath}\")\n",
    "            torch.save(model.state_dict(), _model_filepath)\n",
    "\n",
    "    writer.close()\n",
    "        # # Validation\n",
    "\n",
    "train_valid(\n",
    "    train_dir = \"C:\\\\Users\\\\ook\\\\Documents\\\\dev\\\\ashenvenus\\\\data\\\\split_train\\\\1\",\n",
    "    valid_dir = \"C:\\\\Users\\\\ook\\\\Documents\\\\dev\\\\ashenvenus\\\\data\\\\split_valid\\\\1\",\n",
    "    model = \"vit_b\",\n",
    "    weights_filepath = \"C:\\\\Users\\\\ook\\\\Documents\\\\dev\\\\segment-anything\\\\models\\\\sam_vit_b_01ec64.pth\",\n",
    "    num_samples_train = 64,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
